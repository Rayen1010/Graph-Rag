{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Load and preprocess text data**\n",
    "\n",
    " The first step is to load and preprocess the text data from which we'll extract the knowledge\n",
    " graph. In this example, we'll use a text snippet describing a technology company called\n",
    " prismaticAI, its employees, and their roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load text from a file\n",
    "file_path = \"text.txt\"  # Replace with your actual file path\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ documents = loader.load()\n",
    "What it does:\n",
    "loader.load() reads the contents of the text file and stores them as a list of Document objects.\n",
    "Each Document contains a .page_content attribute that holds the text and a .metadata attribute that stores metadata (like file path).\n",
    "output exmpl : [Document(page_content=\"This is the text from your file...\", metadata={\"source\": \"your_text_file.txt\"})]\n",
    "\n",
    "Why Use loader.load()?\n",
    "loader.load() is necessary because it reads the text file and converts it into a Document object that LangChain can process.\n",
    "Without calling loader.load(), you won't have any text data to work with inside LangChain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "What it does:\n",
    "Creates a CharacterTextSplitter object that will be used to split the text into chunks.\n",
    "chunk_size=200 â†’ Each chunk will have a maximum of 200 characters.\n",
    "chunk_overlap=20 â†’ Each chunk will overlap by 20 characters with the previous one.\n",
    "Why is chunking needed?\n",
    "Large texts can be too big for processing in memory or passing to models.\n",
    "Splitting keeps context while preventing input limits from being exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ texts = text_splitter.split_documents(documents)\n",
    "What it does:\n",
    "\n",
    "Splits the text inside each Document into multiple smaller chunks.\n",
    "Returns a list of new Document objects, each containing a chunk of text.\n",
    "\n",
    "output exmpl : Original text: \"This is an example of a long text document that needs to be split...\"\n",
    "\n",
    "Chunk 1: \"This is an example of a long text document that needs to be split...\"\n",
    "Chunk 2 (overlapping): \"...document that needs to be split into smaller parts...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Initialize language model and extract knowledge graph**\n",
    "\n",
    "After loading and preprocessing the text data, the next step is to initialize a language model and use it to extract a knowledge graph from the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "\n",
    "# Instantiate the Ollama LLM with Llama 3.2\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "# Extract Knowledge Graph\n",
    "llm_transformer = LLMGraphTransformer(llm=model)\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works**\n",
    "\n",
    "1) Processing: LLM-Based Graph Extraction\n",
    "LLMGraphTransformer reads the text and identifies entities (people, companies, roles, etc.).\n",
    "It extracts relationships (who works where, what a company does, etc.).\n",
    "It converts the extracted knowledge into graph documents.\n",
    "\n",
    "2) Output: Graph Representation\n",
    "After transformation, the knowledge graph is stored in graph_documents, structured as nodes and edges.\n",
    "\n",
    "ðŸ”¹ Nodes (Entities)\n",
    "Sarah\n",
    "Michael\n",
    "PrismaticAI\n",
    "Software Engineer\n",
    "Data Scientist\n",
    "AI Solutions\n",
    "ðŸ”¹ Edges (Relationships)\n",
    "Sarah works at PrismaticAI\n",
    "Michael works at PrismaticAI\n",
    "Michael has role Data Scientist\n",
    "Sarah has role Software Engineer\n",
    "PrismaticAI develops AI Solutions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ“Š Schema of the Knowledge Graph**\n",
    "\n",
    "        +----------+        works at        +-------------+\n",
    "        |  Sarah   | ---------------------> | PrismaticAI |\n",
    "        +----------+                        +-------------+\n",
    "             |                                      |\n",
    "       has role                                develops\n",
    "             |                                      |\n",
    "    +----------------+                      +----------------+\n",
    "    | Software Eng. |                      | AI Solutions  |\n",
    "    +----------------+                      +----------------+\n",
    "\n",
    "        +----------+        works at        +-------------+\n",
    "        | Michael  | ---------------------> | PrismaticAI |\n",
    "        +----------+                        +-------------+\n",
    "             |\n",
    "       has role\n",
    "             |\n",
    "    +----------------+\n",
    "    | Data Scientist |\n",
    "    +----------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Store knowledge graph in a database**\n",
    "\n",
    "After extracting the knowledge graph from the text data, it's important to store it in a persistent and queryable format. In this tutorial, we'll use Neo4j to store the knowledge graph. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access the variables like this:\n",
    "neo4j_username = os.getenv(\"NEO4J_USERNAME\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Store Knowledge Graph in Neo4j\n",
    "graph_store = Neo4jGraph(url=\"neo4j+s://a737671e.databases.neo4j.io\", username=neo4j_username, password=neo4j_password)\n",
    "graph_store.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Retrieve knowledge for RAG**\n",
    "\n",
    "Now that we have stored the knowledge graph in a database, we can set up the components for retrieving relevant knowledge from the graph based on user queries and generating responses using the retrieved context. This is the core functionality of a RAG application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "# Retrieve Knowledge for RAG\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(storage_context=graph_store.storage_context, verbose=True)\n",
    "query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever)\n",
    "\n",
    "#Problem : 'Neo4jGraph' object has no attribute 'storage_context'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Query the knowledge graph and generate a response**\n",
    "Finally, we can query the knowledge graph and generate responses using the retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesis import ResponseSynthesizer\n",
    "\n",
    "def query_and_synthesize(query):\n",
    "    retrieved_context = query_engine.query(query)\n",
    "    response = response_synthesizer.synthesize(query, retrieved_context)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {response}\\n\")\n",
    "\n",
    "# Initialize the ResponseSynthesizer instance\n",
    "response_synthesizer = ResponseSynthesizer(model)\n",
    "\n",
    "# Query 1\n",
    "query_and_synthesize(\"Where does Sarah work?\")\n",
    "\n",
    "# Query 2\n",
    "query_and_synthesize(\"Who works for prismaticAI?\")\n",
    "\n",
    "# Query 3\n",
    "query_and_synthesize(\"Does Michael work for the same company as Sarah?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_based",
   "language": "python",
   "name": "ollama_based"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
